{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Workflow\n",
    "\n",
    "\n",
    "Lesson Goals\n",
    "\n",
    "In this lesson you will learn:\n",
    "\n",
    "    The procedure to apply Machine Learning.\n",
    "    The inputs and outputs of every stage of the procedure.\n",
    "    The role of experiments in Machine Learning.\n",
    "\n",
    "Introduction\n",
    "\n",
    "At this moment you may be wondering how to apply Machine Learning to a problem. This lesson provides the answers by breaking down the process into stages and explaining each stage, including the expected inputs and outputs. The Machine Learning workflow is composed of a series of subprocesses, or stages, so that the output of one subprocess is fed as input to the next subprocess. A helpful visual image is a pipeline connecting multiple processing machines.\n",
    "\n",
    "Let's consider as a running example for this lesson an application to eHealth: a project for mining eHRs (electronic Health Records) to improve the quality and effectiveness of health care. More concretely, let's consider the problem of predicting the outcome of a medical treatment prescribed to a patient. Every patient is represented by an instance composed of a set of attributes describing the patient, for example: age, gender, height, weight, risk factors, medication, daily dosage, amount of intakes per day, dosage per intake, lowest blood pressure value, highest blood presure value, etc.\n",
    "\n",
    "You can think of a patient as a vector of values, which are the independent variables. We use Machine Learning to learn a model to estimate the value of the dependent variable \"efficacy of treatment\" that we can model as a real variable in the range [0, 1] with 0 meaning that the treatment is totally ineffective, and 1 meaning totally effective. To evaluate the estimation produced by Machine Learning, we can compare the estimated value to the actual value, and consider the estimation as accurate enough if the difference is 3% at most.\n",
    "\n",
    "In this lesson we will demonstrate the different stages of a machine learning workflow using the census dataset used in previous lessons.\n",
    "ETL\n",
    "\n",
    "ETL stands for Extraction, Transformation and Loading of data. And it is a preliminary process that has to be completed before proceeding to the application of Machine Learning.\n",
    "\n",
    "In the Extraction phase, data is imported from data sources to a data storage with a single unified view, like a Data Warehouse. The data sources may be heterogeneous and distributed. For instance, in our eHealth example, the medical history of a patient may be distributed across different databases belonging to the same or different health systems.\n",
    "\n",
    "In the Transformation phase, the extracted data is homogenized. This includes transforming the data in the following ways:\n",
    "\n",
    "   **Conversion:** magnitudes expressed in different units are all converted to a single reference unit. For instance, all volume magnitudes are converted to milliliters (ml).\n",
    "\n",
    "   **Remove outliers:** an outlier is a data point that clearly does not belong to the distribution of the rest of the dataset. In our eHealth example we can consider that an age of 150 years is certainly an outlier not belonging to the distribution of a representative sample of the general population. Whatever Machine Learning learns from the analysis of this instance is highly likely to not be applicable to the rest of the population. So the outlier is removed. There is another reason to remove the outlier from the data quality perspective. As we will see in the next bullet point, scaling the range of the age variable so that the highest age is mapped to 1, would make the rest of the mapped values to be inadequately compressed.\n",
    "\n",
    "Recall our census dataset. Let's look at the dataset using the describe function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "census=pd.read_csv('../census.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CensusId</th>\n",
       "      <th>TotalPop</th>\n",
       "      <th>Men</th>\n",
       "      <th>Women</th>\n",
       "      <th>Hispanic</th>\n",
       "      <th>White</th>\n",
       "      <th>Black</th>\n",
       "      <th>Native</th>\n",
       "      <th>Asian</th>\n",
       "      <th>Pacific</th>\n",
       "      <th>...</th>\n",
       "      <th>Walk</th>\n",
       "      <th>OtherTransp</th>\n",
       "      <th>WorkAtHome</th>\n",
       "      <th>MeanCommute</th>\n",
       "      <th>Employed</th>\n",
       "      <th>PrivateWork</th>\n",
       "      <th>PublicWork</th>\n",
       "      <th>SelfEmployed</th>\n",
       "      <th>FamilyWork</th>\n",
       "      <th>Unemployment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3220.000000</td>\n",
       "      <td>3.220000e+03</td>\n",
       "      <td>3.220000e+03</td>\n",
       "      <td>3.220000e+03</td>\n",
       "      <td>3220.000000</td>\n",
       "      <td>3220.000000</td>\n",
       "      <td>3220.000000</td>\n",
       "      <td>3220.000000</td>\n",
       "      <td>3220.000000</td>\n",
       "      <td>3220.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3220.000000</td>\n",
       "      <td>3220.000000</td>\n",
       "      <td>3220.000000</td>\n",
       "      <td>3220.000000</td>\n",
       "      <td>3.220000e+03</td>\n",
       "      <td>3220.000000</td>\n",
       "      <td>3220.000000</td>\n",
       "      <td>3220.000000</td>\n",
       "      <td>3220.000000</td>\n",
       "      <td>3220.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>31393.605280</td>\n",
       "      <td>9.940935e+04</td>\n",
       "      <td>4.889694e+04</td>\n",
       "      <td>5.051241e+04</td>\n",
       "      <td>11.011522</td>\n",
       "      <td>75.428789</td>\n",
       "      <td>8.665497</td>\n",
       "      <td>1.723509</td>\n",
       "      <td>1.229068</td>\n",
       "      <td>0.082733</td>\n",
       "      <td>...</td>\n",
       "      <td>3.323509</td>\n",
       "      <td>1.612733</td>\n",
       "      <td>4.631770</td>\n",
       "      <td>23.278758</td>\n",
       "      <td>4.559352e+04</td>\n",
       "      <td>74.219348</td>\n",
       "      <td>17.560870</td>\n",
       "      <td>7.931801</td>\n",
       "      <td>0.288106</td>\n",
       "      <td>8.094441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>16292.078954</td>\n",
       "      <td>3.193055e+05</td>\n",
       "      <td>1.566813e+05</td>\n",
       "      <td>1.626620e+05</td>\n",
       "      <td>19.241380</td>\n",
       "      <td>22.932890</td>\n",
       "      <td>14.279122</td>\n",
       "      <td>7.253115</td>\n",
       "      <td>2.633079</td>\n",
       "      <td>0.734931</td>\n",
       "      <td>...</td>\n",
       "      <td>3.756096</td>\n",
       "      <td>1.670988</td>\n",
       "      <td>3.178772</td>\n",
       "      <td>5.600466</td>\n",
       "      <td>1.496995e+05</td>\n",
       "      <td>7.863188</td>\n",
       "      <td>6.510354</td>\n",
       "      <td>3.914974</td>\n",
       "      <td>0.455137</td>\n",
       "      <td>4.096114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1001.000000</td>\n",
       "      <td>8.500000e+01</td>\n",
       "      <td>4.200000e+01</td>\n",
       "      <td>4.300000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>6.200000e+01</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>19032.500000</td>\n",
       "      <td>1.121800e+04</td>\n",
       "      <td>5.637250e+03</td>\n",
       "      <td>5.572000e+03</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>64.100000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>2.700000</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>4.550750e+03</td>\n",
       "      <td>70.500000</td>\n",
       "      <td>13.100000</td>\n",
       "      <td>5.400000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>5.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>30024.000000</td>\n",
       "      <td>2.603500e+04</td>\n",
       "      <td>1.293200e+04</td>\n",
       "      <td>1.305700e+04</td>\n",
       "      <td>3.900000</td>\n",
       "      <td>84.100000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>3.900000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>1.050800e+04</td>\n",
       "      <td>75.700000</td>\n",
       "      <td>16.200000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>7.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>46105.500000</td>\n",
       "      <td>6.643050e+04</td>\n",
       "      <td>3.299275e+04</td>\n",
       "      <td>3.348750e+04</td>\n",
       "      <td>9.825000</td>\n",
       "      <td>93.200000</td>\n",
       "      <td>9.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>26.800000</td>\n",
       "      <td>2.863275e+04</td>\n",
       "      <td>79.700000</td>\n",
       "      <td>20.500000</td>\n",
       "      <td>9.400000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>9.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>72153.000000</td>\n",
       "      <td>1.003839e+07</td>\n",
       "      <td>4.945351e+06</td>\n",
       "      <td>5.093037e+06</td>\n",
       "      <td>99.900000</td>\n",
       "      <td>99.800000</td>\n",
       "      <td>85.900000</td>\n",
       "      <td>92.100000</td>\n",
       "      <td>41.600000</td>\n",
       "      <td>35.300000</td>\n",
       "      <td>...</td>\n",
       "      <td>71.200000</td>\n",
       "      <td>39.100000</td>\n",
       "      <td>37.200000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>4.635465e+06</td>\n",
       "      <td>88.300000</td>\n",
       "      <td>66.200000</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>9.800000</td>\n",
       "      <td>36.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           CensusId      TotalPop           Men         Women     Hispanic  \\\n",
       "count   3220.000000  3.220000e+03  3.220000e+03  3.220000e+03  3220.000000   \n",
       "mean   31393.605280  9.940935e+04  4.889694e+04  5.051241e+04    11.011522   \n",
       "std    16292.078954  3.193055e+05  1.566813e+05  1.626620e+05    19.241380   \n",
       "min     1001.000000  8.500000e+01  4.200000e+01  4.300000e+01     0.000000   \n",
       "25%    19032.500000  1.121800e+04  5.637250e+03  5.572000e+03     1.900000   \n",
       "50%    30024.000000  2.603500e+04  1.293200e+04  1.305700e+04     3.900000   \n",
       "75%    46105.500000  6.643050e+04  3.299275e+04  3.348750e+04     9.825000   \n",
       "max    72153.000000  1.003839e+07  4.945351e+06  5.093037e+06    99.900000   \n",
       "\n",
       "             White        Black       Native        Asian      Pacific  ...  \\\n",
       "count  3220.000000  3220.000000  3220.000000  3220.000000  3220.000000  ...   \n",
       "mean     75.428789     8.665497     1.723509     1.229068     0.082733  ...   \n",
       "std      22.932890    14.279122     7.253115     2.633079     0.734931  ...   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "25%      64.100000     0.500000     0.100000     0.200000     0.000000  ...   \n",
       "50%      84.100000     1.900000     0.300000     0.500000     0.000000  ...   \n",
       "75%      93.200000     9.600000     0.600000     1.200000     0.000000  ...   \n",
       "max      99.800000    85.900000    92.100000    41.600000    35.300000  ...   \n",
       "\n",
       "              Walk  OtherTransp   WorkAtHome  MeanCommute      Employed  \\\n",
       "count  3220.000000  3220.000000  3220.000000  3220.000000  3.220000e+03   \n",
       "mean      3.323509     1.612733     4.631770    23.278758  4.559352e+04   \n",
       "std       3.756096     1.670988     3.178772     5.600466  1.496995e+05   \n",
       "min       0.000000     0.000000     0.000000     4.900000  6.200000e+01   \n",
       "25%       1.400000     0.900000     2.700000    19.500000  4.550750e+03   \n",
       "50%       2.400000     1.300000     3.900000    23.000000  1.050800e+04   \n",
       "75%       4.000000     1.900000     5.600000    26.800000  2.863275e+04   \n",
       "max      71.200000    39.100000    37.200000    44.000000  4.635465e+06   \n",
       "\n",
       "       PrivateWork   PublicWork  SelfEmployed   FamilyWork  Unemployment  \n",
       "count  3220.000000  3220.000000   3220.000000  3220.000000   3220.000000  \n",
       "mean     74.219348    17.560870      7.931801     0.288106      8.094441  \n",
       "std       7.863188     6.510354      3.914974     0.455137      4.096114  \n",
       "min      25.000000     5.800000      0.000000     0.000000      0.000000  \n",
       "25%      70.500000    13.100000      5.400000     0.100000      5.500000  \n",
       "50%      75.700000    16.200000      6.900000     0.200000      7.600000  \n",
       "75%      79.700000    20.500000      9.400000     0.300000      9.900000  \n",
       "max      88.300000    66.200000     36.600000     9.800000     36.500000  \n",
       "\n",
       "[8 rows x 35 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "census.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CensusId', 'State', 'County', 'TotalPop', 'Men', 'Women', 'Hispanic',\n",
       "       'White', 'Black', 'Native', 'Asian', 'Pacific', 'Citizen', 'Income',\n",
       "       'IncomeErr', 'IncomePerCap', 'IncomePerCapErr', 'Poverty',\n",
       "       'ChildPoverty', 'Professional', 'Service', 'Office', 'Construction',\n",
       "       'Production', 'Drive', 'Carpool', 'Transit', 'Walk', 'OtherTransp',\n",
       "       'WorkAtHome', 'MeanCommute', 'Employed', 'PrivateWork', 'PublicWork',\n",
       "       'SelfEmployed', 'FamilyWork', 'Unemployment'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "census.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3220 entries, 0 to 3219\n",
      "Data columns (total 37 columns):\n",
      "CensusId           3220 non-null int64\n",
      "State              3220 non-null object\n",
      "County             3220 non-null object\n",
      "TotalPop           3220 non-null int64\n",
      "Men                3220 non-null int64\n",
      "Women              3220 non-null int64\n",
      "Hispanic           3220 non-null float64\n",
      "White              3220 non-null float64\n",
      "Black              3220 non-null float64\n",
      "Native             3220 non-null float64\n",
      "Asian              3220 non-null float64\n",
      "Pacific            3220 non-null float64\n",
      "Citizen            3220 non-null int64\n",
      "Income             3219 non-null float64\n",
      "IncomeErr          3219 non-null float64\n",
      "IncomePerCap       3220 non-null int64\n",
      "IncomePerCapErr    3220 non-null int64\n",
      "Poverty            3220 non-null float64\n",
      "ChildPoverty       3219 non-null float64\n",
      "Professional       3220 non-null float64\n",
      "Service            3220 non-null float64\n",
      "Office             3220 non-null float64\n",
      "Construction       3220 non-null float64\n",
      "Production         3220 non-null float64\n",
      "Drive              3220 non-null float64\n",
      "Carpool            3220 non-null float64\n",
      "Transit            3220 non-null float64\n",
      "Walk               3220 non-null float64\n",
      "OtherTransp        3220 non-null float64\n",
      "WorkAtHome         3220 non-null float64\n",
      "MeanCommute        3220 non-null float64\n",
      "Employed           3220 non-null int64\n",
      "PrivateWork        3220 non-null float64\n",
      "PublicWork         3220 non-null float64\n",
      "SelfEmployed       3220 non-null float64\n",
      "FamilyWork         3220 non-null float64\n",
      "Unemployment       3220 non-null float64\n",
      "dtypes: float64(27), int64(8), object(2)\n",
      "memory usage: 930.9+ KB\n"
     ]
    }
   ],
   "source": [
    "census.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look for outliers in our census data. Even though the census data is thoroughly cleaned before it is released, for the sake of this exercise, let's pick the FamilyWork feature in our dataset. This column show the percent of people in each county that primarily perform unpaid family work. We can plot the boxplot of the feature to see how many points are extreme outliers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAHCCAYAAAC63V38AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAdW0lEQVR4nO3df5DcdZ3n8dc7PckMJKwR0QGCRyRx3Z6MwrrjrouzyfQNLMtyB/GOVVPrFS6tEeucs4hsJtq353JVvTCUdRUuWkBiB1G5vjvZJQRB1l/derOhPCcL3Jn0smQ9VMQgirBODGGm531/dCckIQmZmc73+56Z56MqNdPf6Znv65+uVz6f7+f7+Zq7CwCAaOalHQAAgGOhoAAAIVFQAICQKCgAQEgUFAAgJAoKABBSW5InO+uss3zp0qVJnhIIad++fVq4cGHaMYAQdu7c+XN3f/3RxxMtqKVLl2pkZCTJUwIhVatV9fX1pR0DCMHMfnis40zxAQBCoqAAACFRUACAkCgoAEBIr1pQZrbVzH5mZt8/7NiZZvZ1M3ui+fW1pzYmAGCuOZkR1Ocl/dFRxzZI+qa7v1nSN5uvAQBomVctKHf/jqTnjjp8laS7mt/fJWl1i3MBAOa4qV6D6nT3n0pS8+sbWhcJAIAEbtQ1s7WS1kpSZ2enqtXqqT4lEN7o6CifBeBVTLWgnjGzc9z9p2Z2jqSfHe+N7r5Z0mZJ6unpce6eB9hJAjgZU53i2y7pmub310i6rzVxAABoOJll5mVJD0t6i5k9ZWZ5STdLutTMnpB0afM1AAAtczKr+Na4+znuPt/dz3P3krv/wt373f3Nza9Hr/IDcAzlclnd3d3q7+9Xd3e3yuVy2pGAsBLdzRyYy8rlsgqFgkqlkur1ujKZjPL5vCRpzZo1KacD4mGrIyAhxWJRpVJJuVxObW1tyuVyKpVKKhaLaUcDQqKggITUajX19vYecay3t1e1Wi2lREBsFBSQkGw2q+Hh4SOODQ8PK5vNppQIiI2CAhJSKBSUz+dVqVQ0Pj6uSqWifD6vQqGQdjQgJBZJAAk5uBBiYGBAtVpN2WxWxWKRBRLAcZi7J3aynp4eHxkZSex8QFTsJAG8zMx2unvP0ceZ4gMAhERBAQBCoqAAACFRUACAkCgoAEBIFBQAICQKCgAQEgUFAAiJggIAhERBAQBCoqAAACFRUACAkCgoAEBIFBQAICQKCgAQEgUFAAiJggIAhERBAQBCoqAAACFRUACAkCgoAEBIFBQAICQKCgAQEgUFAAiJggIAhERBAQBCoqAAACFRUACAkCgoAEBIFBQAICQKCgAQEgUFAAiJggIAhERBAQBCoqAAACFRUACAkCgoAEBIFBQAICQKCgAQEgUFAAiJggIAhERBAQBCoqAAACFRUACAkCgoAEBIFBQAICQKCgAQEgUFAAiJggIAhERBAQBCoqAAACFRUACAkCgoAEBIFBQAICQKCgAQ0rQKysyuN7NdZvZ9MyubWUerggGzUblcVnd3t/r7+9Xd3a1yuZx2JCCstqn+opktkfQfJHW5+34z+5+S3ifp8y3KBswq5XJZhUJBpVJJ9XpdmUxG+XxekrRmzZqU0wHxTHeKr03SaWbWJul0SU9PPxIwOxWLRZVKJeVyObW1tSmXy6lUKqlYLKYdDQhpyiMod/+JmX1a0o8k7Zf0NXf/2tHvM7O1ktZKUmdnp6rV6lRPCcxotVpN9Xpd1WpVo6OjqlarqtfrqtVqfC6AY5jOFN9rJV0l6U2Snpf0ZTN7v7t/6fD3uftmSZslqaenx/v6+qaeFpjBstmsMpmM+vr6VK1W1dfXp0qlomw2Kz4XwCtNZ4rvEkn/z92fdfcxSX8j6eLWxAJmn0KhoHw+r0qlovHxcVUqFeXzeRUKhbSjASFNeQSlxtTeO83sdDWm+PoljbQkFTALHVwIMTAwoFqtpmw2q2KxyAIJ4DjM3af+y2Y3SnqvpHFJj0j6oLsfON77e3p6fGSEDgMOTvEBkMxsp7v3HH18OiMoufunJH1qOn8DAIBjYScJAEBIFBQAICQKCgAQEgUFAAiJggIAhERBAQBCoqAAACFRUACAkCgoAEBIFBQAICQKCgAQEgUFAAiJggIAhERBAQBCoqAAACFRUACAkCgoAEBIFBQAICQKCgAQEgUFAAiJggIAhERBAQBCoqAAACFRUACAkCgoAEBIFBQAICQKCgAQEgUFAAiJggIAhERBAQBCoqAAACFRUACAkCgoAEBIFBQAICQKCgAQEgUFAAiJggIAhERBAQBCoqAAACFRUACAkCgoAEBIFBQAICQKCgAQEgUFAAiJggIAhERBAQBCoqAAACFRUACAkCgoAEBIFBQAICQKCgAQEgUFAAiJggIAhERBAQBCoqAAACFRUACAkCgoAEBIFBQAICQKCkhQuVxWd3e3+vv71d3drXK5nHYkIKy2tAMAc0W5XFahUFCpVFK9Xlcmk1E+n5ckrVmzJuV0QDyMoICEFItFlUol5XI5tbW1KZfLqVQqqVgsph0NCImCAhJSq9XU29t7xLHe3l7VarWUEgGxTWuKz8wWS/qcpG5JLulad3+4FcGA2SabzerGG2/Utm3bVKvVlM1mtXr1amWz2bSjASFN9xrUrZIecverzWyBpNNbkAmYlXK5nIaGhjQ0NKSuri7t3r1bg4ODuu6669KOBoQ05YIys9+QtFLSByTJ3V+S9FJrYgGzT6VS0eDgoLZu3XpoBDU4OKht27alHQ0IaTojqAskPSvpTjO7UNJOSR9z932Hv8nM1kpaK0mdnZ2qVqvTOCUwc9VqNW3cuFGXXHKJRkdHtWjRIo2Pj+umm27icwEcw3QKqk3S2yUNuPt3zexWSRsk/cXhb3L3zZI2S1JPT4/39fVN45TAzJXNZpXJZNTX16dqtaq+vj5VKhVls1nxuQBeaTqr+J6S9JS7f7f5+h41CgvAMRQKBeXzeVUqFY2Pj6tSqSifz6tQKKQdDQhpyiMod99rZj82s7e4++OS+iXtbl00YHY5eDPuwMDAoWtQxWKRm3SB4zB3n/ovm12kxjLzBZJ+IOnP3P2Xx3t/T0+Pj4yMTPl8wGxxcIoPgGRmO9295+jj01pm7u6PSnrFHwUAYLrYSQIAEBIFBQAIiYICAIREQQEAQqKgAAAhUVAAgJAoKABASBQUACAkCgoAEBIFBQAIiYICAIREQQEAQqKgAAAhUVAAgJAoKABASBQUACAkCgoAEBIFBQAIiYICAIREQQEAQqKgAAAhUVAAgJAoKABASBQUACAkCgoAEBIFBQAIiYICAIREQQEAQqKgAAAhUVAAgJAoKABASBQUACAkCgoAEBIFBQAIiYICAIREQQEAQqKgAAAhUVAAgJAoKABASBQUACAkCgoAEBIFBQAIiYICAIREQQEAQqKggASVy2V1d3erv79f3d3dKpfLaUcCwmpLOwAwV5TLZRUKBZVKJdXrdWUyGeXzeUnSmjVrUk4HxMMICkhIsVhUqVRSLpdTW1ubcrmcSqWSisVi2tGAkCgoICG1Wk29vb1HHOvt7VWtVkspERAbBQUkJJvNanh4+Ihjw8PDymazKSUCYqOggIQUCgXl83lVKhWNj4+rUqkon8+rUCikHQ0IiUUSQEIOLoQYGBhQrVZTNptVsVhkgQRwHObuiZ2sp6fHR0ZGEjsfEFW1WlVfX1/aMYAQzGynu/ccfZwpPgBASBQUACAkCgoAEBIFBQAIiYICAIREQQEAQqKgAAAhUVAAgJAoKABASNMuKDPLmNkjZvaVVgQCAEBqzQjqY5J4XgBwEniiLnDyprVZrJmdJ+kKSUVJ61qSCJileKIuMDnTHUFtlLRe0kQLsgCzGk/UBSZnyiMoM/tXkn7m7jvNrO8E71sraa0kdXZ2qlqtTvWUwIxWq9VUr9dVrVY1OjqqarWqer2uWq3G5wI4hulM8b1L0pVm9seSOiT9hpl9yd3ff/ib3H2zpM1S43EbPGIAc1U2m1Umk1FfX9+hx21UKhVls1kevQEcw5Sn+Nz9E+5+nrsvlfQ+Sd86upwAvIwn6gKTwxN1gYTwRF1gclpSUO5elVRtxd8CAEBiBAUkhmXmwOSw1RGQEJaZA5NDQQEJqdVq6u3tPeJYb2+vajU2YgGOhYICEpLNZjU8PHzEseHhYWWz2ZQSAbFRUEBCWGYOTA6LJICEsMwcmBxGUACAkBhBAQlhmTkwOYyggISwzByYHAoKSAjLzIHJoaCAhLDMHJgcCgpICMvMgclhkQSQEJaZA5PDCAoAEBIjKCAhLDMHJocRFJAQlpkDk0NBAQlhmTkwORQUkBCWmQOTQ0EBCWGZOTA5LJIAEsIyc2ByGEEBCdqxY4f27NmjiYkJ7dmzRzt27Eg7EhAWIyggIQMDA7r99ts1NDSkrq4u7d69W4ODg5KkTZs2pZwOiIcRFJCQLVu2aGhoSOvWrVNHR4fWrVunoaEhbdmyJe1oQEiMoICEHDhwQI8//rg6Ojp04MABtbe365prrtGBAwfSjgaEREEBCclkMtqyZYs+/elPH5riu+GGG5TJZNKOBoTEFB+QEHeXmR1xzMzk7iklAmJjBAUkZGJiQh/+8If1yU9+8tAU34c+9CHdcccdaUcDQmIEBSSkvb1do6OjWr58uebNm6fly5drdHRU7e3taUcDQqKggISsWrVKd999t1auXKn77rtPK1eu1N13361Vq1alHQ0IiSk+ICE/+clPtHr1am3dulW33Xab2tvbtXr1aj3xxBNpRwNCoqCAhNRqNT3yyCOaP3++qtWq+vr6NDY2po6OjrSjASExxQckhN3MgcmhoICEsJs5MDlM8QEJYTdzYHIsyZsEe3p6fGRkJLHzAVEdvAYFQDKzne7ec/RxpvgAACFRUACAkCgoIEHlclnd3d3q7+9Xd3e3yuVy2pGAsFgkASSkXC6rUCioVCqpXq8rk8kon89LEgslgGNgBAUkpFgsqlQqKZfLqa2tTblcTqVSScViMe1oQEgUFJCQWq2m3t7eI4719vaqVqullAiIjSk+ICHZbFbvec979NWvfvXQ4zYuv/xydpIAjoMRFJCQJUuWaNu2bbr22mt1//3369prr9W2bdu0ZMmStKMBIXGjLpCQjo4OXX311Xr00UcP7SRx0UUX6Z577tGLL76YdjwgNce7UZeCAhJiZtq3b59OP/30QztJ/PrXv9bChQt57DvmtOMVFNeggIS0t7dr2bJl2rt376FjZ599Nk/UBY6Da1BAQhYuXKi9e/dqxYoVKpfLWrFihfbu3auFCxemHQ0IiREUkJDnnntOS5cu1Z49e7RmzRq1t7dr6dKlevLJJ9OOBoTECApI0Pr167V8+XLNmzdPy5cv1/r169OOBITFCApI0Mc//nE98MADh7Y6uuKKK9KOBITFCApISHt7u/bv36+NGzdqdHRUGzdu1P79+1kkARwHIyggIWNjY+ru7tb27du1fft2SVJ3d7d2796dcjIgJgoKSEg2m9VVV10ldz90o+7B1wBeiYICEpLL5TQ0NKShoSF1dXVp9+7dGhwc1HXXXZd2NCAkCgpISKVS0eDgoLZu3XpoBDU4OKht27alHQ0Iia2OgIRkMhm9+OKLmj9//qGtjsbGxtTR0aF6vZ52PCA1x9vqiFV8QEKy2ayGh4ePODY8PMzjNoDjoKCAhBQKBeXzeVUqFY2Pj6tSqSifz6tQKKQdDQiJa1BAQtasWSNJGhgYOHQNqlgsHjoO4EiMoIAE7dixQ3v27NHExIT27NmjHTt2pB0JCIsRFJCQgYEB3X777a9YZi5JmzZtSjkdEA8jKCAhW7Zs0dDQkNatW6eOjg6tW7dOQ0ND2rJlS9rRgJCmXFBm9kYzq5hZzcx2mdnHWhkMmG0OHDigrVu3ysyUy+VkZtq6dasOHDiQdjQgpOmMoMYlfdzds5LeKenfm1lXa2IBs4+ZadeuXbryyit177336sorr9SuXbtkZmlHA0KackG5+0/d/e+b3/9KUk3SklYFA2Ybd5eZadWqVero6NCqVatkZuzFBxxHSxZJmNlSSb8t6bvH+NlaSWslqbOzU9VqtRWnBGakyy67TBs2bNDY2Jjmz5+vyy67TA899BCfC+AYpr3VkZktkvRtSUV3/5sTvZetjjCXmZnmz5+vsbGxQ8cOvmYUhbnslGx1ZGbzJf21pLtfrZyAuW7evHkaGxvTokWLdNttt2nRokUaGxvTvHkspgWOZcpTfNa4sluSVHP3/9K6SMDsNDExoQULFmh0dFQf+chHJEkLFizQSy+9lHIyIKbp/NftXZL+naR/aWaPNv/9cYtyAbPSmWeeecLXAF42nVV8w+5u7v42d7+o+e/BVoYDZpu9e/fq4osv1pe//GVdfPHF2rt3b9qRgLCY/AYS9sILL2h8fFwvvPBC2lGA0NiLD0jQ4sWLtWvXrkM7mC9evFjPP/98yqmAmBhBAQnat2/fCV8DeBkFBSSEZebA5DDFByRkYmJC8+fPP2KZ+dE37gJ4Gf91AwCEREEBCRobG1NnZ6fuvPNOdXZ2MnoCToCCAhK2bNkyLVq0SMuWLUs7ChAa16CABHV2dmrHjh3asWPHodfPPPNMyqmAmBhBAQk6uowoJ+D4KCggBQMDA2lHAMKjoIAUbNq0Ke0IQHgUFJCgTCZzwtcAXkZBAQmq1+tHLDOv1+tpRwLCoqCAhD3zzDN6+umnWSABvAoKCkhBoVBIOwIQHgUFAAiJggJSsH79+rQjAOFRUEAKbrnllrQjAOFRUACAkCgoIAUf/OAH044AhEdBAQnLZDLq6uriJl3gVbCbOZCwer2udevWpR0DCI8RFAAgJAoKSMGFF16YdgQgPAoKSMFjjz2WdgQgPAoKSNDQ0JDcXZVKRe6uoaGhtCMBYZm7J3aynp4eHxkZSex8QCRmdtyfJfk5BKIxs53u3nP0cUZQQApWrlyZdgQgPAoKSMF3vvOdtCMA4VFQAICQKCggBVdffXXaEYDwKCggBStWrEg7AhAeBQWk4MYbb0w7AhAeBQUACImCAlJw9tlnpx0BCI+CAlKwd+/etCMA4VFQAICQKCggBV1dXWlHAMKjoIAUnHvuuWlHAMKjoIAUfOMb30g7AhAeBQUk7PDHbQA4vra0AwBzjZmpq6tLu3fvTjsKEBojKCAhh4+YDi8nRlLAsTGCAqbhRA8hPJV/g1LDXMAICpgGd5/Sv/MHvzLl36WcMFdQUACAkCgoAEBIFBQAICQKCgAQEgUFAAiJggIAhERBAQBCoqAAACGxkwTmvAtv/Jpe2D+W+HmXbngg8XO+5rT5euxTf5j4eYGpoKAw572wf0xP3nxFouesVqvq6+tL9JxSOqUITBVTfACAkCgoAEBITPFhzjsju0FvvWtD8ie+K/lTnpGVpGSnM4GpoqAw5/2qdjPXoICAmOIDAIQ0rRGUmf2RpFslZSR9zt1vbkkqIGGpjCweSmeZOTBTTLmgzCwj6bOSLpX0lKTvmdl2d9994t8EYkl6ek9qFGIa5wVmkulM8f2upD3u/gN3f0nSf5d0VWtiAQDmuukU1BJJPz7s9VPNYwAATNt0rkHZMY75K95ktlbSWknq7OxUtVqdximBWHK53JR/14amft5KpTL1XwZmiOkU1FOS3njY6/MkPX30m9x9s6TNktTT0+NpLK0FThX3V/yf7KSktcwcmEmmM8X3PUlvNrM3mdkCSe+TtL01sQAAc92UR1DuPm5mH5X0t2osM9/q7rtalgwAMKdN6z4od39Q0oMtygIAwCHsJAEACImCAgCEREEBAEKioAAAIVFQAICQKCgAQEgUFAAgJAoKABASBQUACImCAgCEREEBAEKioAAAIdlUn2czpZOZPSvph4mdEIjrLEk/TzsEEMT57v76ow8mWlAAGsxsxN170s4BRMYUHwAgJAoKABASBQWkY3PaAYDouAYFAAiJERQAICQKCgAQEgUFAAiJggJmIDOz5tf2tLMApwoFBcxA7u5m1ifpuoNlBcw2FBQwQ5hZp5k9bGanNw9dIGmiWVaUFGYdCgqYIdz9GUk/lvS/zew0SSZpYfNn3C+CWYf7oIAZwMza3H28+f0dkn5H0heaP/4/kn4pqUPSr9x9dzopgdZqSzsAgBMzM3P38eY1p4K7X2pmn5G0UdK3JL1R0jJJiyWtSy8p0FoUFBBc8xrTOyV9RNJfNY991MxGJa1290skycwWuftoilGBluIaFDAzXCDpTyT9i4MH3H2DpMfMbI+ZtUk6kFY44FRgBAUE1JzWczN7jaQX3f2/mdkCSQUz+5G7VyTJ3d9rZm89eH0KmE1YJAEEY2YZd6+b2ZWS/kzSayR9SdL/kvQ2SQVJg+7+9eb7jVV8mI0YQQFBmNnr1FiF95KZdUm6SdKfSspKWiHpXEm3SnqtpFvNrFfSLyknzFZcgwICaN58+0k1CkmSzpH0A3d/1N3Lkh6QlJP0m+7+OUl/6O7PUU6YzSgoIIZxSc9Lenvz9aNqzN69W5Lc/WFJNTVGU3L3p9IICSSJggJSZGZLzOzt7v6SpP8q6ffN7GZ3/4Uao6ZeM/vPZvYHki6X9HiaeYEksUgCSElzWu9OSb8t6UPu/m0zO1/SF9W43+nbknrVmPabkHSvu9+fVl4gaRQUkCIz+6ikDZK+KmmnpH9WY/HSUkmfbY6kZGanuft+VuxhLmGKD0iYmb3BzK6QJHf/jKTPSqpLelKNa1BrJV0k6R0Hf8fd9ze/Uk6YMygoIEFmNl+NHSGuN7Nbm4f/Wo0FEE+5+w2SvidplaS7zOx0HqWBuYopPiAFZnaBGruR/0CNgnqXpJq739n8+ZWSnm2u3gPmJAoKSJiZvU1Svxq7kRfVeK5Tlxo34/65u9+bYjwgDKb4gIQcNlV3hqS3NK8n/UdJ29VYsXeBpI+a2WKm9QC2OgJOKTOb5+4TzZftkl5UY1rvrWZ2rbtvlfSwpIfN7BeSnnT351OKC4TCCAo4RczsbElXmdkZZvZGSV80s4vd/aeSrpf0TjN7vZllJMnd72reC8XoCRAjKOBU+l1J16jxOXtC0jck3Whm/7d5rEPSme7+7OEjLZaSAw0UFNBiZtYp6R3uvt3MFkn615IedPc7zOx+SZ1qPDLj9ySdYWbvbW51BOAwFBTQej2S/snMzmg+aPCApH/bnLl7yN0fMbP3S/odSe9X4zEaT6aWFgiKggJazN0fMLPFkm4xs0fcfXOznN4tacLMvuXuP5f0d2b2F2qMpJ5MLzEQEwUFtMjh++S5+/NmtkPS75nZB9z982Y2ocaIKWNm29RYbn6uGo/WAHAUCgpoEXd3M7tU0m9J+gd3/6KZjUq6zMwm3P0LzRV7TzT31ttvZr3u/s+pBgeCoqCAFjGzFZI+o8ZznN5hZn/g7v+pOXJ6t5m1Ne97Ovz+qF+lGBkIja2OgGk4OK1nZudJuljSr939K2Z2kRr3Oj3p7p8ys38j6R/d/fupBgZmEAoKmKLDyulySX8p6UxJX1ejmF6S9DY1lpPvcfdPphYUmKHYSQKYomY59Uj6gBqLH9aqcf3pTyRl3P0xSTdJ+h+phQRmMK5BAVPUvAn3TyX9vqQfu/sTzce4Xy+p3czucvdHUg0JzGCMoIBJOHyfPHcflXSrpL+X9JnmY9kfkLRJjVFVZyohgVmCa1DASTrsmtNlkpapMY23yczOl/SJ5tuud/f9ZvZ6d382vbTAzMcICjhJzXK6QtItkv5B0g1mdoe7/1CNBw+ersZIap6k51KMCswKjKCAEzCz35T0Jnf/WzN7raQ71RgtvVnSn0t6g6TH3P09zZHUGSwlB1qDggKOo1lO2yTd4u6fbx47R9LrJH1BjT30XifpaUlfdPdrUooKzEqs4gOOwcy6JN0t6RPufl9z2u48d/+RmZ2lxsIISTpf0pAaz3oC0EJcgwKO7UxJF7r7fc3XD0i6uvn9hBrXmzZJukfSA+7+TZ6EC7QWU3zAcTR3iPispH+S9Hfu/peH/ewiSa+RVHf34XQSArMbBQWcgJn1S3pI0oLmKr7TmjuRAzjFmOIDTsDdvynpKkn/aGZnNe9x4totkAA+aMCrcPcHzawuaZeZ/Za7/zLtTMBcwBQfcJKaN+nuc/dq2lmAuYCCAibp8Ee7Azh1KCgAQEgskgAAhERBAQBCoqAAACFRUACAkCgoAEBIFBQAIKT/D5MNZ1OSEXZ9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "census.boxplot(column=['FamilyWork'])\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, quite a bit of data is considered extreme outliers in this boxplot. To get rid of these outliers, we typically remove the entire row that contains the outlier. We compute the outer fence which is 3 times the IQR from the first and third quartiles. We then remove the outliers using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CensusId</th>\n",
       "      <th>State</th>\n",
       "      <th>County</th>\n",
       "      <th>TotalPop</th>\n",
       "      <th>Men</th>\n",
       "      <th>Women</th>\n",
       "      <th>Hispanic</th>\n",
       "      <th>White</th>\n",
       "      <th>Black</th>\n",
       "      <th>Native</th>\n",
       "      <th>...</th>\n",
       "      <th>Walk</th>\n",
       "      <th>OtherTransp</th>\n",
       "      <th>WorkAtHome</th>\n",
       "      <th>MeanCommute</th>\n",
       "      <th>Employed</th>\n",
       "      <th>PrivateWork</th>\n",
       "      <th>PublicWork</th>\n",
       "      <th>SelfEmployed</th>\n",
       "      <th>FamilyWork</th>\n",
       "      <th>Unemployment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>55221</td>\n",
       "      <td>26745</td>\n",
       "      <td>28476</td>\n",
       "      <td>2.6</td>\n",
       "      <td>75.8</td>\n",
       "      <td>18.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.8</td>\n",
       "      <td>26.5</td>\n",
       "      <td>23986</td>\n",
       "      <td>73.6</td>\n",
       "      <td>20.9</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1003</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Baldwin</td>\n",
       "      <td>195121</td>\n",
       "      <td>95314</td>\n",
       "      <td>99807</td>\n",
       "      <td>4.5</td>\n",
       "      <td>83.1</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>26.4</td>\n",
       "      <td>85953</td>\n",
       "      <td>81.5</td>\n",
       "      <td>12.3</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1005</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Barbour</td>\n",
       "      <td>26932</td>\n",
       "      <td>14497</td>\n",
       "      <td>12435</td>\n",
       "      <td>4.6</td>\n",
       "      <td>46.2</td>\n",
       "      <td>46.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>24.1</td>\n",
       "      <td>8597</td>\n",
       "      <td>71.8</td>\n",
       "      <td>20.8</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>17.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1007</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Bibb</td>\n",
       "      <td>22604</td>\n",
       "      <td>12073</td>\n",
       "      <td>10531</td>\n",
       "      <td>2.2</td>\n",
       "      <td>74.5</td>\n",
       "      <td>21.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>28.8</td>\n",
       "      <td>8294</td>\n",
       "      <td>76.8</td>\n",
       "      <td>16.1</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1009</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Blount</td>\n",
       "      <td>57710</td>\n",
       "      <td>28512</td>\n",
       "      <td>29198</td>\n",
       "      <td>8.6</td>\n",
       "      <td>87.9</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>34.9</td>\n",
       "      <td>22189</td>\n",
       "      <td>82.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>7.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   CensusId    State   County  TotalPop    Men  Women  Hispanic  White  Black  \\\n",
       "0      1001  Alabama  Autauga     55221  26745  28476       2.6   75.8   18.5   \n",
       "1      1003  Alabama  Baldwin    195121  95314  99807       4.5   83.1    9.5   \n",
       "2      1005  Alabama  Barbour     26932  14497  12435       4.6   46.2   46.7   \n",
       "3      1007  Alabama     Bibb     22604  12073  10531       2.2   74.5   21.4   \n",
       "4      1009  Alabama   Blount     57710  28512  29198       8.6   87.9    1.5   \n",
       "\n",
       "   Native  ...  Walk  OtherTransp  WorkAtHome  MeanCommute  Employed  \\\n",
       "0     0.4  ...   0.5          1.3         1.8         26.5     23986   \n",
       "1     0.6  ...   1.0          1.4         3.9         26.4     85953   \n",
       "2     0.2  ...   1.8          1.5         1.6         24.1      8597   \n",
       "3     0.4  ...   0.6          1.5         0.7         28.8      8294   \n",
       "4     0.3  ...   0.9          0.4         2.3         34.9     22189   \n",
       "\n",
       "   PrivateWork  PublicWork  SelfEmployed  FamilyWork  Unemployment  \n",
       "0         73.6        20.9           5.5         0.0           7.6  \n",
       "1         81.5        12.3           5.8         0.4           7.5  \n",
       "2         71.8        20.8           7.3         0.1          17.6  \n",
       "3         76.8        16.1           6.7         0.4           8.3  \n",
       "4         82.0        13.5           4.2         0.4           7.7  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "#census=census[pd.notnull(census.Income)]\n",
    "\n",
    "q1 = np.percentile(census.FamilyWork, 25)\n",
    "q3 = np.percentile(census.FamilyWork, 75)\n",
    "iqr = q3 - q1\n",
    "upper_fence = q3 + 3 * iqr\n",
    "lower_fence = q1 - 3 * iqr\n",
    "#census_without_outliers = census[census.FamilyWork < upper_fence & census.FamilyWork > lower_fence]\n",
    "\n",
    "print (len(census[census.FamilyWork < lower_fence]))\n",
    "census_without_outliers=census[census.FamilyWork < upper_fence]\n",
    "census_without_outliers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our initial dataset has 3220 rows and after removing outliers we have only 3022 rows. A total of 198 rows were removed.\n",
    "\n",
    "   **Scaling:** all variables are scaled so that they take values from the same range, typically this range is [0, 1] (also known as Normalization). This is done to avoid bias effects in error metrics. For instance, consider variable A whose range is [0, 1] and variable B whose range is [0, 100]. A 10% error in the measurement of A would cause a \"noise\" of 0.01, but the same percentage of error in variable B would add a \"noise\" of 10. Thus the overall error might be dominated by errors in B due to scaling reasons, but not necessarily because of the right reason (possibly B has more influence in estimating the right value). Note that if we use an algorithm to scale the training data, then we will have to scale input data with the same algorithm before inputting it to the model learned by Machine Learning.\n",
    "\n",
    "One thing we note in our dataset is that many columns in this dataset are expressed as a percent of the county population. These columns contain a number between 0 and 100. It might be easier for our calculations if we used a number between 0 and 1 and divided all these columns by 100.\n",
    "\n",
    "We can do this one column at a time, for example, here we convert the Hispanic column: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "census['HispanicRate'] = census['Hispanic'] / 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this is not very efficient. It is more efficient to convery all columns at once. First we make a list of the columns to be converted and then convert them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.185\n",
       "1    0.095\n",
       "2    0.467\n",
       "3    0.214\n",
       "4    0.015\n",
       "Name: BlackRate, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_percent(x):\n",
    "    return(x/100)\n",
    "\n",
    "conversion_list = ['Hispanic', 'White', 'Black', 'Native', 'Asian', 'Pacific', 'Poverty', 'ChildPoverty', 'Professional', 'Service', 'Office', 'Construction', 'Production', 'Drive', 'Carpool', 'Transit', 'Walk', 'OtherTransp', 'WorkAtHome', 'Employed', 'PrivateWork', 'PublicWork']\n",
    "\n",
    "#we create a new list of columns. We prefer doing this since we'd rather keep the old data around just in case.\n",
    "\n",
    "new_column_list = [x+'Rate' for x in conversion_list]\n",
    "census[new_column_list] = census[conversion_list].apply(to_percent)\n",
    "\n",
    "#Let's look at the Black rate to be sure this worked\n",
    "\n",
    "census.BlackRate.head()                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "   **Rounding:** all real values are rounded to the same amount of decimal figures. For instance 0.333333333333 becomes 0.3333.\n",
    "\n",
    "Rounding can be done using the round function in Python.\n",
    "\n",
    "We can round one of the rate columns that we generated. For example, we can round the NativeRate column to reduce the decimal places to 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.004\n",
       "1    0.006\n",
       "2    0.002\n",
       "3    0.004\n",
       "4    0.003\n",
       "Name: NativeRateRounded, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "census['NativeRateRounded'] = round(census['NativeRate'], 3)\n",
    "census.NativeRateRounded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "   **Standardization:** transform the data so that it has zero mean (mean removal) and unit variance (variance rescaling). For instance, w = (x-mean) / sigma.\n",
    "\n",
    "We can demonstrate this by standardizing the population size in each county. Scikit-learn (which is a library that we will introduce in greater detail in subsequent lessons) has a function that performs this task called MinMaxScaler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Javier\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    3220.000000\n",
       "mean        0.009895\n",
       "std         0.031809\n",
       "min         0.000000\n",
       "25%         0.001109\n",
       "50%         0.002585\n",
       "75%         0.006609\n",
       "max         1.000000\n",
       "Name: TotalPopScaled, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#This code uses the reshape function to change the input into a two dimensional numpy array \n",
    "#since this is the input that the function takes.\n",
    "census['TotalPopScaled'] = MinMaxScaler().fit_transform(census.TotalPop.values.reshape(-1,1))\n",
    "census.TotalPopScaled.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "   **Dummy Variables/One Hot Encoding:** In order to use categorical variables in our models, we perform a transformation that is known in statistics as creating dummy variables and as one hot encoding in computer science. The main idea is to add a variable to the dataset for each value of the categorical variable but one. These variables then take a value of either zero or one to indicate whether the original feature was equal to the value in the new dummy variable column. For example, in the census dataset, we can create a dummy variable for the state feature. We will create a variable for every state and territory but one. The reason for this is that since only one of the dummy variables can be 1 for each row, this means that we can exactly predict the value of one column using the sum of all the rest. This will add a perfectly correlated column to our dataset. As we have shown in previous lessons, this scenario should be avoided.\n",
    "\n",
    "Here is the code for creating a dummy variable for the State feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CensusId', 'State', 'County', 'TotalPop', 'Men', 'Women',\n",
       "       'Hispanic', 'White', 'Black', 'Native', 'Asian', 'Pacific',\n",
       "       'Citizen', 'Income', 'IncomeErr', 'IncomePerCap',\n",
       "       'IncomePerCapErr', 'Poverty', 'ChildPoverty', 'Professional',\n",
       "       'Service', 'Office', 'Construction', 'Production', 'Drive',\n",
       "       'Carpool', 'Transit', 'Walk', 'OtherTransp', 'WorkAtHome',\n",
       "       'MeanCommute', 'Employed', 'PrivateWork', 'PublicWork',\n",
       "       'SelfEmployed', 'FamilyWork', 'Unemployment', 'HispanicRate',\n",
       "       'WhiteRate', 'BlackRate', 'NativeRate', 'AsianRate', 'PacificRate',\n",
       "       'PovertyRate', 'ChildPovertyRate', 'ProfessionalRate',\n",
       "       'ServiceRate', 'OfficeRate', 'ConstructionRate', 'ProductionRate',\n",
       "       'DriveRate', 'CarpoolRate', 'TransitRate', 'WalkRate',\n",
       "       'OtherTranspRate', 'WorkAtHomeRate', 'EmployedRate',\n",
       "       'PrivateWorkRate', 'PublicWorkRate', 'NativeRateRounded',\n",
       "       'TotalPopScaled', 'Alaska', 'Arizona', 'Arkansas', 'California',\n",
       "       'Colorado', 'Connecticut', 'Delaware', 'District of Columbia',\n",
       "       'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana',\n",
       "       'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland',\n",
       "       'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi',\n",
       "       'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire',\n",
       "       'New Jersey', 'New Mexico', 'New York', 'North Carolina',\n",
       "       'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania',\n",
       "       'Puerto Rico', 'Rhode Island', 'South Carolina', 'South Dakota',\n",
       "       'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington',\n",
       "       'West Virginia', 'Wisconsin', 'Wyoming'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states = pd.get_dummies(census.State, drop_first=True)\n",
    "census_dummy = pd.concat([census, states], axis=1)\n",
    "census_dummy.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "   **Shuffling:** when working with datasets contained in files (for instance ARFF files or CSV files), it makes sense to shuffle the rows. Otherwise other operations that will happen later in this workflow might be biased. In our eHealth example, if we have the dataset ordered by gender, train the model on the first half, and test it with the second, we would be applying the knowledge learned from the female population to the male population, which might bias the estimations.\n",
    "\n",
    "In Pandas, we can shuffle rows using the sample function. We can take a random sample where we select 100% of our original data (sample proportion = 1). This will result in a random ordering of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_census = census.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the dataset has been transformed, the Loading phase is executed to store the resulting dataset into a data store, for instance a database.\n",
    "\n",
    "\n",
    "**Sampling**\n",
    "\n",
    "After we have performed ETL on the raw data, we need to split the transformed dataset. At this point you have the option to apply Machine Learning in different ways (that will be explained in the following sections). The option that you choose to use is called your \"Experimental Design\". Depending on this experimental design you will need to split your transformed dataset in different ways. For the time being, it is enough to know about the simplest experimental design, known as train-test split. The train-test split means that the transformed dataset is divided into two disjoint subsets: one subset that will be used for training Machine Learning (known as the Training Set), and another to test the model learned by Machine Learning (known as the Test Set).\n",
    "\n",
    "Note that the sampling used to select the train set and the test set must be random. If the transformed dataset is contained in a text file, it is very helpful to have this file shuffled during ETL (as mentioned in the preceding section).\n",
    "\n",
    "Sampling is done using the sample function. Using this function, we can determine the number of rows we want in our sample or the proportion of the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample using proportion\n",
    "census_prop_sample = census.sample(frac=0.5)\n",
    "#sample using number of rows\n",
    "census_size_sample = census.sample(n=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Holdout Set**\n",
    "\n",
    "A Holdout Set is a subset that we obtain from the transformed dataset and that is not available to Machine Learning during the training phase (this phase is explained in the next section). In the case of the train-test split, the subset used to test the Machine Learning model is a holdout set known as the Test Set. And it is very important that this test set not be available to Machine Learning during training.\n",
    "\n",
    "\n",
    "**Training**\n",
    "\n",
    "After the Training Set is available as a result of the completion of the sampling phase, we are ready to put Machine Learning to work. Our first step is to select the Machine Learning algorithm. After selecting the Machine Learning algorithm, we train it on the Training Set.\n",
    "\n",
    "If we want to perform a train test split, there is a specific function to perform this task called train_test_split in Scikit-learn. Using this function, we pick what proportion of the data will be in the training set and the remainder will be in the test set. Typically we like to have the majority of the dataset in our training set. This is because the more data we use for training, the more accurate our model will be.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CensusId</th>\n",
       "      <th>State</th>\n",
       "      <th>County</th>\n",
       "      <th>TotalPop</th>\n",
       "      <th>Men</th>\n",
       "      <th>Women</th>\n",
       "      <th>Hispanic</th>\n",
       "      <th>White</th>\n",
       "      <th>Black</th>\n",
       "      <th>Native</th>\n",
       "      <th>...</th>\n",
       "      <th>CarpoolRate</th>\n",
       "      <th>TransitRate</th>\n",
       "      <th>WalkRate</th>\n",
       "      <th>OtherTranspRate</th>\n",
       "      <th>WorkAtHomeRate</th>\n",
       "      <th>EmployedRate</th>\n",
       "      <th>PrivateWorkRate</th>\n",
       "      <th>PublicWorkRate</th>\n",
       "      <th>NativeRateRounded</th>\n",
       "      <th>TotalPopScaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>8021</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>Conejos</td>\n",
       "      <td>8249</td>\n",
       "      <td>4111</td>\n",
       "      <td>4138</td>\n",
       "      <td>54.4</td>\n",
       "      <td>43.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.057</td>\n",
       "      <td>30.21</td>\n",
       "      <td>0.658</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.000813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2548</th>\n",
       "      <td>48051</td>\n",
       "      <td>Texas</td>\n",
       "      <td>Burleson</td>\n",
       "      <td>17293</td>\n",
       "      <td>8611</td>\n",
       "      <td>8682</td>\n",
       "      <td>19.7</td>\n",
       "      <td>66.2</td>\n",
       "      <td>12.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.026</td>\n",
       "      <td>73.52</td>\n",
       "      <td>0.694</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2775</th>\n",
       "      <td>48505</td>\n",
       "      <td>Texas</td>\n",
       "      <td>Zapata</td>\n",
       "      <td>14308</td>\n",
       "      <td>7153</td>\n",
       "      <td>7155</td>\n",
       "      <td>93.6</td>\n",
       "      <td>6.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.023</td>\n",
       "      <td>51.84</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>17013</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>Calhoun</td>\n",
       "      <td>4999</td>\n",
       "      <td>2528</td>\n",
       "      <td>2471</td>\n",
       "      <td>0.3</td>\n",
       "      <td>97.8</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.052</td>\n",
       "      <td>22.31</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3122</th>\n",
       "      <td>56007</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Carbon</td>\n",
       "      <td>15739</td>\n",
       "      <td>8592</td>\n",
       "      <td>7147</td>\n",
       "      <td>17.4</td>\n",
       "      <td>78.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.054</td>\n",
       "      <td>78.51</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.001559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CensusId     State    County  TotalPop   Men  Women  Hispanic  White  \\\n",
       "255       8021  Colorado   Conejos      8249  4111   4138      54.4   43.3   \n",
       "2548     48051     Texas  Burleson     17293  8611   8682      19.7   66.2   \n",
       "2775     48505     Texas    Zapata     14308  7153   7155      93.6    6.4   \n",
       "601      17013  Illinois   Calhoun      4999  2528   2471       0.3   97.8   \n",
       "3122     56007   Wyoming    Carbon     15739  8592   7147      17.4   78.4   \n",
       "\n",
       "      Black  Native  ...  CarpoolRate  TransitRate  WalkRate  OtherTranspRate  \\\n",
       "255     0.1     1.6  ...        0.127        0.006     0.025            0.010   \n",
       "2548   12.3     0.2  ...        0.134        0.001     0.008            0.006   \n",
       "2775    0.0     0.0  ...        0.192        0.003     0.034            0.003   \n",
       "601     0.1     0.1  ...        0.133        0.006     0.014            0.017   \n",
       "3122    1.4     0.6  ...        0.107        0.004     0.042            0.015   \n",
       "\n",
       "      WorkAtHomeRate  EmployedRate  PrivateWorkRate  PublicWorkRate  \\\n",
       "255            0.057         30.21            0.658           0.213   \n",
       "2548           0.026         73.52            0.694           0.217   \n",
       "2775           0.023         51.84            0.733           0.181   \n",
       "601            0.052         22.31            0.770           0.128   \n",
       "3122           0.054         78.51            0.725           0.218   \n",
       "\n",
       "      NativeRateRounded  TotalPopScaled  \n",
       "255               0.016        0.000813  \n",
       "2548              0.002        0.001714  \n",
       "2775              0.000        0.001417  \n",
       "601               0.001        0.000490  \n",
       "3122              0.006        0.001559  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "census_train, census_test = train_test_split(census, test_size = 0.2)\n",
    "census_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CensusId</th>\n",
       "      <th>State</th>\n",
       "      <th>County</th>\n",
       "      <th>TotalPop</th>\n",
       "      <th>Men</th>\n",
       "      <th>Women</th>\n",
       "      <th>Hispanic</th>\n",
       "      <th>White</th>\n",
       "      <th>Black</th>\n",
       "      <th>Native</th>\n",
       "      <th>...</th>\n",
       "      <th>CarpoolRate</th>\n",
       "      <th>TransitRate</th>\n",
       "      <th>WalkRate</th>\n",
       "      <th>OtherTranspRate</th>\n",
       "      <th>WorkAtHomeRate</th>\n",
       "      <th>EmployedRate</th>\n",
       "      <th>PrivateWorkRate</th>\n",
       "      <th>PublicWorkRate</th>\n",
       "      <th>NativeRateRounded</th>\n",
       "      <th>TotalPopScaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3219</th>\n",
       "      <td>72153</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>Yauco</td>\n",
       "      <td>39474</td>\n",
       "      <td>19047</td>\n",
       "      <td>20427</td>\n",
       "      <td>99.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.031</td>\n",
       "      <td>89.23</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.003924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2327</th>\n",
       "      <td>45023</td>\n",
       "      <td>South Carolina</td>\n",
       "      <td>Chester</td>\n",
       "      <td>32556</td>\n",
       "      <td>15902</td>\n",
       "      <td>16654</td>\n",
       "      <td>1.7</td>\n",
       "      <td>59.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.034</td>\n",
       "      <td>122.96</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2702</th>\n",
       "      <td>48359</td>\n",
       "      <td>Texas</td>\n",
       "      <td>Oldham</td>\n",
       "      <td>2071</td>\n",
       "      <td>1137</td>\n",
       "      <td>934</td>\n",
       "      <td>14.4</td>\n",
       "      <td>72.2</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.052</td>\n",
       "      <td>7.73</td>\n",
       "      <td>0.732</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1808</th>\n",
       "      <td>35025</td>\n",
       "      <td>New Mexico</td>\n",
       "      <td>Lea</td>\n",
       "      <td>68149</td>\n",
       "      <td>35007</td>\n",
       "      <td>33142</td>\n",
       "      <td>54.5</td>\n",
       "      <td>39.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.024</td>\n",
       "      <td>293.40</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.006780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2248</th>\n",
       "      <td>42009</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>Bedford</td>\n",
       "      <td>49086</td>\n",
       "      <td>24504</td>\n",
       "      <td>24582</td>\n",
       "      <td>1.1</td>\n",
       "      <td>97.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.041</td>\n",
       "      <td>224.99</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.004881</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CensusId           State   County  TotalPop    Men  Women  Hispanic  \\\n",
       "3219     72153     Puerto Rico    Yauco     39474  19047  20427      99.5   \n",
       "2327     45023  South Carolina  Chester     32556  15902  16654       1.7   \n",
       "2702     48359           Texas   Oldham      2071   1137    934      14.4   \n",
       "1808     35025      New Mexico      Lea     68149  35007  33142      54.5   \n",
       "2248     42009    Pennsylvania  Bedford     49086  24504  24582       1.1   \n",
       "\n",
       "      White  Black  Native  ...  CarpoolRate  TransitRate  WalkRate  \\\n",
       "3219    0.5    0.0     0.0  ...        0.085        0.012     0.016   \n",
       "2327   59.0   38.0     0.2  ...        0.113        0.004     0.017   \n",
       "2702   72.2    6.1     0.5  ...        0.090        0.000     0.039   \n",
       "1808   39.4    3.2     0.7  ...        0.111        0.006     0.020   \n",
       "2248   97.1    0.5     0.2  ...        0.104        0.002     0.025   \n",
       "\n",
       "      OtherTranspRate  WorkAtHomeRate  EmployedRate  PrivateWorkRate  \\\n",
       "3219            0.007           0.031         89.23            0.680   \n",
       "2327            0.019           0.034        122.96            0.800   \n",
       "2702            0.010           0.052          7.73            0.732   \n",
       "1808            0.013           0.024        293.40            0.808   \n",
       "2248            0.016           0.041        224.99            0.810   \n",
       "\n",
       "      PublicWorkRate  NativeRateRounded  TotalPopScaled  \n",
       "3219           0.276              0.000        0.003924  \n",
       "2327           0.156              0.002        0.003235  \n",
       "2702           0.201              0.005        0.000198  \n",
       "1808           0.136              0.007        0.006780  \n",
       "2248           0.108              0.002        0.004881  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "census_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What Happens During Training?**\n",
    "\n",
    "During training, we apply the machine learning algorithm to our data. The algorithm typically comprised of two mathematical equations. The first equation is the mathematical model. The model describes the mathematical relationship between the features in the data. The second part of the algorithm is the loss function. The loss function quantifies how much information was lost using our model by comparing the actual data with the predicted data that is outputted by the model. Our goal is to optimize the loss. Some machine learning algorithms will continue iterating until achieving an optimal loss.\n",
    "\n",
    "The output of the training phase is a trained model that incorporates the knowledge learned by Machine Learning, which can be queried to solve unseen problems (new problems different from those present in the training set).\n",
    "\n",
    "\n",
    "**Testing**\n",
    "\n",
    "Now, you might be wondering \"how good is the model for solving new problems?\" This is the question that the testing phase answers by computing the value of a quality metric. Usually you will have a threshold value for this quality metric, meaning that for the model to be usefull the value of its quality metric must be equal or greater to the threshold value.\n",
    "\n",
    "It is important to note that the test set cannot be used for training. The fact that Machine Learning is trained with a training set of solved problems (supervised Machine Learning), and the fact that it solves those problems well, does not provide any useful information on how well it generalizes (how well it solves new problems).\n",
    "\n",
    "\n",
    "**Experimental Design**\n",
    "\n",
    "Machine Learning is an experimental science. This means that when you approach a new problem, you don't know beforehand what Machine Learning algorithm will solve the problem satisfactorily. So you have to try different algorithms and analyze what happens. Every time you perform one of these trials you are actually making a Machine Learning experiment. That is why it is said that Machine Learning is an experimental science.\n",
    "\n",
    "Before you perform any Machine Learning experiment, you have to think about how you will proceed. This is what is called the experimental design. Next, you will learn about the most common expèrimental designs that you can try out in solving Machine Learning problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross Validation**\n",
    "\n",
    "There is a probability that the train-test split tests the model on the only subset where it performs well, thus providing an unreliable quality metric. To reduce this probability, you can test on many different test sets and compute an average of the individual quality metrics obtained. This is what cross validation does. The procedure is:\n",
    "\n",
    "    Randomly partition the dataset in n bins.\n",
    "    For every bin ( b):\n",
    "        Train with the remaining ( n-1) bins.\n",
    "        Test with b and obtain quality metric.\n",
    "    Output the average of the n quality metrics obtained.\n",
    "\n",
    "\n",
    "\n",
    "**Train-Validation-Test Split**\n",
    "\n",
    "The training of some Machine Learning algorithms may be interpreted as learning a set of parameters that minimize the estimation error of the resulting model. How these parameters are learned is controlled by another set of parameters, that are called hyper parameters to avoid the confusion between both sets of parameters. Typical examples of this kind of Machine Learning paradigms are SVMs (Support Vector Machines) and Artificial Neural Networks.\n",
    "\n",
    "The basic procedure is:\n",
    "\n",
    "    Randomly partition the dataset in 3 disjoint subsets (training, validation, and test sets).\n",
    "    Initialize the hyperparameters\n",
    "    Train with the train set.\n",
    "    Evaluate on validation set.\n",
    "    Update best performing hyperparameters.\n",
    "    Recompute/modify the hyperparameters while maximum number of iterations not reached.\n",
    "    Using the best performing hyperparameters, compute the definitive quality metric on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
